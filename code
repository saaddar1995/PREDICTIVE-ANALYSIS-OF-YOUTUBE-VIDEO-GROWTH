# -*- coding: utf-8 -*-
"""Dar_SD_13117533_2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DIguxtPPSz30hchM-1gD9fGRZ8h5KoJM
"""

###################### Code For Fetching Youtube DATA Using Data API #################################

!pip install schedule # for scheduling

#Youtube data api Refrence : https://developers.google.com/youtube/v3/docs

# below code fetching youtube data video data and save in data frame
import pandas as pd
import schedule
import time
from googleapiclient.discovery import build

api_key = 'AIzaSyAWYQRfFQUqPHStRrwxnUUvd2HJX_B1Vr8'  # API key from Google Console

youtube = build('youtube', 'v3', developerKey=api_key)

video_id = 'QN_gFsF_sCg' #add youtube id


video_details_df = pd.DataFrame(columns=['Video ID', 'Title', 'Views', 'Likes', 'Date'])

def fetch_video_details():
    try:
        # by using YouTube Data API fetching video details features
        video_response = youtube.videos().list(
            part='snippet,statistics',
            id=video_id
        ).execute()

        # Extracting the video details BY indexing
        video_title = video_response['items'][0]['snippet']['title']
        video_views = video_response['items'][0]['statistics']['viewCount']
        video_likes = video_response['items'][0]['statistics']['likeCount']
        current_date = pd.Timestamp.now().strftime('%Y-%m-%d')

        # Append the details to the DataFrame
        global video_details_df
        video_details_df = video_details_df.append({'Video ID': video_id, 'Title': video_title, 'Views': video_views, 'Likes': video_likes, 'Date': current_date}, ignore_index=True)

    except Exception as e:
        print('! Code Not Running due to ERROR !')
        print(e)

    # Refrence : https://www.geeksforgeeks.org/python-schedule-library/
    # Stop scheduling after 30 minutes  and save DataFrame to CSV
    if time.time() - start_time >= 1800:
        schedule.clear()
        video_details_df.to_csv('video_details.csv', index=False)
        print('Data saved to video_details.csv')
        return


    print('Video Details:')
    print(video_details_df)

start_time = time.time()


schedule.every(20).seconds.do(fetch_video_details)


while time.time() - start_time < 1800:
    schedule.run_pending()
    time.sleep(1)

video_details_df

c

#below code is combining two csv and save in one file
import pandas as pd

# This is Existing DataFrame
df = pd.read_csv('combined_data35.csv')

# New data to be added which we extract
new_data = pd.read_csv('/content/video_details.csv')

# Append the new data to the existing DataFrame in one file
combined_df = df.append(new_data, ignore_index=True)

# Save the combined DataFrame to a new CSV file format
combined_df.to_csv('combined_data36.csv', index=False)

##################################### till here we fetch data by using youtube data api #####################################
###################################################################################################################################################

#+++++++++++++++++++++++++++ below code is for extracting required rows +++++++++++++++++++++++++++

import pandas as pd


data = pd.read_csv('/content/Data set Youtube Video.csv')
df1 = pd.DataFrame(data)

df1

import pandas as pd


data = pd.read_csv('/content/Data set Youtube Video.csv')
df = pd.DataFrame(data)

# Select every 91st rows
selected_rows = df.iloc[::91]

selected_rows

selected_rows.shape

selected_rows['Views'] = selected_rows['Views'].diff()

only_data_views = selected_rows[['Date','Views']]

only_data_views

import pandas as pd

# Assuming you have a DataFrame called video_details_df

# Convert the DataFrame to a CSV file
only_data_views.to_csv('YouTube_Data.csv', index=False)

#+++++++++++++++++++++++++++++++++++++++++++++ END OF EXTRACTING DATA CODE +++++++++++++++++++++++++++++++++++++++++

# Commented out IPython magic to ensure Python compatibility.
#MAIN Code Start Frome Here
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

df=pd.read_csv('/content/Data set Youtube Video.csv')

df.head()

#df_cleaned = df.dropna(inplace=True)

df

df.info()

df.head()

# Convert Month into Datetime
df['Date']=pd.to_datetime(df['Date'])

# Replacing index 0,1,2 to date
df.set_index('Date',inplace=True)

df.head()

df.describe()

df.plot()

### Testing data is it Stationarity or non stationary
from statsmodels.tsa.stattools import adfuller

test_result=adfuller(df['Views'])

import pandas as pd
from statsmodels.tsa.stattools import adfuller

##checking data is stationary or not  https://github.com/krishnaik06/ARIMA-And-Seasonal-ARIMA/blob/master/Untitled.ipynb
def check_stationarity(Views):
    result = adfuller(Views)

    print("ADF Test Statistic:", result[0])
    print("p-value:", result[1])
    print("#Lags Used:", result[2])
    print("Number of Observations Used:", result[3])

    if result[1] <= 0.05:
        print("Strong evidence against the null hypothesis (Ho). Data has no unit root and is stationary.")
    else:
        print("Weak evidence against null hypothesis. Time series has a unit root, indicating it is non-stationary.")

# Assuming Views is your time series data
check_stationarity(df['Views'])

#finding autocoerrlation in Views
import pandas as pd
import matplotlib.pyplot as plt



pd.plotting.autocorrelation_plot(df['Views'])
plt.show()

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf

#below code is to find paramter values by using ACF AND PACF
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm



fig = plt.figure(figsize=(12, 8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(df['Views'].iloc[13:], lags=100, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(df['Views'].iloc[13:], lags=80, ax=ax2)

plt.show()

from statsmodels.tsa.arima_model import ARIMA

import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA


views = df['Views']

model = ARIMA(views.values, order=(1, 1, 1))
model_fit = model.fit()

model_fit.summary()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm

# we already defined and fitted the ARIMA model as 'model_fit'
start_index = 200
end_index = 350

# Predicting data
forecast = model_fit.predict(start=start_index, end=end_index, dynamic=True)

# New column with NaN values
df['forecast'] = np.nan

# Assigning the forecasted values to the appropriate rows in the 'forecast' column
df['forecast'].iloc[start_index:end_index + 1] = forecast

# Plot actual sales and forecasted values
df[['Views', 'forecast']].plot(figsize=(12, 8))
plt.show()

# Calculate RMSE for the forecasted period
actual_values = df['Views'].iloc[start_index:end_index + 1]
mse = np.mean((forecast - actual_values)**2)
rmse = np.sqrt(mse)

print("ARIMA Root Mean Squared Error (RMSE) for the forecasted period:", rmse)

#############___ now applying SARIMAX

import statsmodels.api as sm

model=sm.tsa.statespace.SARIMAX(df['Views'], order=(1, 1, 0), seasonal_order=(1,1,0,15))
results=model.fit()

df['forecast']=results.predict(start=250, end=350,dynamic=True)
df[['Views','forecast']].plot(figsize=(12,8))

actual_values = df['Views'].iloc[250:351]
forecasted_values = df['forecast'].iloc[250:351]
mse = np.mean((forecasted_values - actual_values) ** 2)
rmse = np.sqrt(mse)
plt.show()
print(" SARIMAX ROOT Mean Squared Error (RMSE) for the forecasted period:", rmse)

results.summary()

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt


views = df['Views']

# SARIMAX model and fit it to the data
model = sm.tsa.statespace.SARIMAX(views, order=(1, 1, 0), seasonal_order=(1, 1, 0, 15))
results = model.fit()

# here i define range for date
prediction_start = '2023-04-04'
prediction_end = '2023-07-07'

# Getting predicted values and confidence intervals
forecast = results.get_prediction(start=pd.to_datetime(prediction_start), end=pd.to_datetime(prediction_end), dynamic=True)
predicted_values = forecast.predicted_mean
confidence_int = forecast.conf_int()

# Plotting predicted values and confidence intervals
plt.figure(figsize=(10, 6))
plt.plot(views.index, views, label='Actual Views')
plt.plot(predicted_values.index, predicted_values, color='red', label='Predicted Views')
plt.fill_between(confidence_int.index, confidence_int.iloc[:, 0], confidence_int.iloc[:, 1], color='gray', alpha=0.3,
                 label='Confidence Interval')
plt.xlabel('Date')
plt.ylabel('Views')
plt.title('SARIMAX Predicted Views with Confidence Interval')
plt.legend()
plt.show()
####https://stackoverflow.com/questions/72808256/plotting-confidence-interval-in-sarimax-prediction-data
#https://stackoverflow.com/questions/64277905/statsmodels-arima-how-to-get-confidence-prediction-interval
#https://www.statsmodels.org/dev/generated/statsmodels.tsa.base.prediction.PredictionResults.html

####### now applying xg boost ######

#---=--=-==-=-=-=-=-=- xg boost applying https://www.youtube.com/watch?v=vV12dGe_Fho

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.metrics import mean_squared_error
color_pal = sns.color_palette()
plt.style.use('fivethirtyeight')

df = pd.read_csv('/content/Data set Youtube Video.csv')
df = df.set_index('Date')
df.index = pd.to_datetime(df.index)

df.plot(style='.',
        figsize=(15, 5),
        color=color_pal[0],
        title='Views in months')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# we  have  DataFrame named 'df' with 'Date' and 'Views' columns
# Make sure to replace 'Date' and 'Views' with the actual column names from your DataFrame.

# Sort the DataFrame by date if not already sorted
df.sort_values(by='Date', inplace=True)

# Create the line plot using Seaborn
plt.figure(figsize=(12, 8))
sns.lineplot(data=df, x='Date', y='Views', color='blue', label='Actual Views',linewidth=1.5)

# Add any additional plot customization here (e.g., title, x-axis label, y-axis label, etc.)
plt.xlabel('Date')
plt.ylabel('Views')
plt.title('Time Series of Views')
plt.legend()

# Show the plot
plt.show()

train = df.loc[df.index < '2023-04-04']
test = df.loc[df.index >= '2023-04-04']
fig, ax = plt.subplots(figsize=(15, 5))
train.plot(ax=ax, label='Training Set', title='Data Train/Test Split', linewidth=1.5)
test.plot(ax=ax, label='Test Set',linewidth=1.5)
ax.axvline('2023-04-04', color='black', ls='--')
ax.legend(['Training Set', 'Test Set'])
plt.show()

import matplotlib.pyplot as plt

# we have already executed the code and obtained the DataFrame 'df'

# Create the plot with minimized line width
df.loc[(df.index > '2023-04-01') & (df.index < '2023-07-08')].plot(figsize=(15, 5), title='Week Of Data', linewidth=1.5)

# Set the title, labels, etc. (if needed, you can further customize the plot)
plt.title('Week Of Data')
plt.xlabel('Date')
plt.ylabel('Value')

# Show the plot
plt.tight_layout()
plt.show()

def create_features(df):
    """
    Create time series features based on time series index.
    """
    df = df.copy()
    df['hour'] = df.index.hour
    df['dayofweek'] = df.index.dayofweek
    df['quarter'] = df.index.quarter
    df['month'] = df.index.month
    df['year'] = df.index.year
    df['dayofyear'] = df.index.dayofyear
    df['dayofmonth'] = df.index.day
    df['weekofyear'] = df.index.isocalendar().week
    return df

df = create_features(df)

df

train = create_features(train)
test = create_features(test)

FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month', 'year']
TARGET = 'Views'

X_train = train[FEATURES]
y_train = train[TARGET]

X_test = test[FEATURES]
y_test = test[TARGET]

y_test

#Applying XGBOOST
reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',
                       n_estimators=1000,
                       early_stopping_rounds=50,
                       objective='reg:linear',
                       max_depth=3,
                       learning_rate=0.001)
reg.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=100)

fi = pd.DataFrame(data=reg.feature_importances_,
             index=reg.feature_names_in_,
             columns=['importance'])
fi.sort_values('importance').plot(kind='barh', title='Feature Importance')
plt.show()

test['prediction'] = reg.predict(X_test)
df = df.merge(test[['prediction']], how='left', left_index=True, right_index=True)
ax = df[['Views']].plot(figsize=(15, 5))
df['prediction'].plot(ax=ax, style=':')
plt.legend(['Truth Data', 'Predictions'])
ax.set_title('Raw Dat and Prediction')
plt.show()

df

import matplotlib.pyplot as plt

# Assuming df is your DataFrame

# Filter the DataFrame for the specified date range
filtered_df = df.loc[(df.index > '2023-04-01') & (df.index < '2023-07-08')]

# Plot Views and Prediction
plt.figure(figsize=(15, 5))
plt.plot(filtered_df.index, filtered_df['Views'], label='Truth Data')
plt.plot(filtered_df.index, filtered_df['prediction'], linestyle='--', label='Prediction')
plt.title('Week Of Data')
plt.legend()
plt.show()

score = np.sqrt(mean_squared_error(test['Views'], test['prediction']))
print(f'RMSE Score on Test set: {score:0.2f}')
